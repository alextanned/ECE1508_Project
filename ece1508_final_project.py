# -*- coding: utf-8 -*-
"""ECE1508 Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pKwdEzUDvcO2sCoG5GtdQJai_0qtcRf_
"""

# Import necessary libraries
import time
import torch.utils
import torch.utils.data
import pandas as pd
import numpy as np
import argparse
import os
import datetime
import wandb
import json
import nlpaug.augmenter.word as naw

import torch
from transformers import AutoTokenizer, AutoModelForMultipleChoice

# Define helper functions and classes
def data_prep(args):
    import nltk
    nltk.download('averaged_perceptron_tagger_eng')

    # Load the JSON files directly into lists of dictionaries
    with open("train.json", "r") as f:
        train_data = json.load(f)
    with open("test.json", "r") as f:
        val_data = json.load(f)

    # Apply augmentation if specified
    if args['aug']:
        new_train_data = []
        # Augment with synonyms using WordNet
        aug = naw.SynonymAug(aug_src='wordnet')
        for sample in train_data:
            new_query = aug.augment(sample['query'])
            if isinstance(new_query, list):
              new_query = new_query[0]
            new_train_data.append({'query': new_query, 'options': sample['options'], 'answer': sample['answer']})

        train_data.extend(new_train_data)

        # # Augment with spelling corrections
        # aug_spelling = naw.SpellingAug()
        # for sample in train_data:
        #     sample['query'] = aug_spelling.augment(sample['query'])

    if args['debug']:
        train_data = train_data[:10]
        val_data = val_data[:10]

    # Determine the number of choices; assuming each sample's 'options' field is a dict.
    num_choices = len(train_data[0]['options'])

    return train_data, val_data, 2

def format_time(elapsed):
    elapsed_rounded = int(round((elapsed)))
    return str(datetime.timedelta(seconds=elapsed_rounded))

def load_model(args, model_name, tokenizer_name, num_choices, output_attentions=False, output_hidden_states=False):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = AutoModelForMultipleChoice.from_pretrained(model_name).to(device)
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    return model, tokenizer

def create_dataset(examples, tokenizer):
    input_ids_list = []
    attention_mask_list = []
    labels_list = []

    # Iterate over each example
    for example in examples:
        query = example['query']
        options = example['options']  # options is a dict where keys are option identifiers
        sorted_keys = sorted(options.keys())
        positive_text = options[example['answer']]  # the correct option

        # For every negative candidate, create one pairwise sample
        for key in sorted_keys:
            if key == example['answer']:
                continue  # Skip the positive candidate here
            negative_text = options[key]

            # Tokenize the positive pair: (query, positive_text)
            pos_encoding = tokenizer(
                query,
                positive_text,
                add_special_tokens=True,
                truncation=True,
                max_length=128,
                padding='max_length',
                return_tensors='pt'
            )

            # Tokenize the negative pair: (query, negative_text)
            neg_encoding = tokenizer(
                query,
                negative_text,
                add_special_tokens=True,
                truncation=True,
                max_length=128,
                padding='max_length',
                return_tensors='pt'
            )

            # Combine the positive and negative encodings into one tensor with shape (2, seq_length)
            # The first index is the positive and the second index is the negative.
            input_ids = torch.cat([pos_encoding['input_ids'], neg_encoding['input_ids']], dim=0)
            attention_mask = torch.cat([pos_encoding['attention_mask'], neg_encoding['attention_mask']], dim=0)

            input_ids_list.append(input_ids)
            attention_mask_list.append(attention_mask)
            labels_list.append(0)  # since the positive is at index 0

    # Stack all samples to form tensors of shape (num_samples, 2, seq_length)
    input_ids_tensor = torch.stack(input_ids_list)
    attention_mask_tensor = torch.stack(attention_mask_list)
    labels_tensor = torch.tensor(labels_list)

    # Create a TensorDataset: each sample is (input_ids, attention_mask, label)
    dataset = torch.utils.data.TensorDataset(input_ids_tensor, attention_mask_tensor, labels_tensor)
    return dataset

def create_dataset_normal(examples, tokenizer):
    input_ids_list = []
    attention_mask_list = []
    labels_list = []
    for example in examples:
        query = example['query']
        options = example['options']
        # Use sorted keys
        sorted_keys = sorted(options.keys())
        choices = [options[key] for key in sorted_keys]

        # Tokenize each (query, option) pair
        tokenized_choices = [
            tokenizer(query, choice, add_special_tokens=True, truncation=True, max_length=128, padding='max_length')
            for choice in choices
        ]
        input_ids = [tc['input_ids'] for tc in tokenized_choices]
        attention_mask = [tc['attention_mask'] for tc in tokenized_choices]

        # Determine the label index based on the key matching the answer.
        label = sorted_keys.index(example['answer'])
        input_ids_list.append(input_ids)
        attention_mask_list.append(attention_mask)
        labels_list.append(label)

    input_ids_tensor = torch.tensor(input_ids_list) # [400, 5, 128] num_samples, options per sample, context length
    attention_mask_tensor = torch.tensor(attention_mask_list) # [400]
    labels_tensor = torch.tensor(labels_list)

    dataset = torch.utils.data.TensorDataset(input_ids_tensor, attention_mask_tensor, labels_tensor)
    return dataset

def get_data_loaders(train_dataset, val_dataset, batch_size):
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset,
        sampler=torch.utils.data.RandomSampler(train_dataset),
        batch_size=batch_size
    )

    val_dataloader = torch.utils.data.DataLoader(
        val_dataset,
        sampler=torch.utils.data.SequentialSampler(val_dataset),
        batch_size=1
    )

    return train_dataloader, val_dataloader

def get_optimizer(model, learning_rate):
    optimizer = torch.optim.AdamW(model.parameters(),
                      lr=learning_rate,
                      eps=1e-8)
    return optimizer

def get_scheduler(dataloader, optimizer, epochs):
    total_steps = len(dataloader) * epochs
    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=total_steps,T_mult=2)
    return scheduler

def accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

def train(args):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    train_data, val_data, num_choices = data_prep(args)

    model, tokenizer = load_model(args, args['model'], args['model'], num_choices,
                                  output_attentions=False, output_hidden_states=False)

    train_dataset = create_dataset(train_data, tokenizer)
    val_dataset = create_dataset_normal(val_data, tokenizer)

    epochs = args['epochs']
    batch_size = args['bs']
    learning_rate = args['lr']

    train_dataloader, validation_dataloader = get_data_loaders(train_dataset, val_dataset, batch_size)

    optimizer = get_optimizer(model, learning_rate)
    scheduler = get_scheduler(train_dataloader, optimizer, epochs)

    total_t0 = time.time()
    best_accuracy = 0
    for epoch_i in range(0, epochs):
        print("")
        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))
        print('Training...')

        total_train_accuracy = 0
        t0 = time.time()
        total_train_loss = 0

        model.train()

        for step, batch in enumerate(train_dataloader):
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)

            model.zero_grad()

            loss, logits = model(b_input_ids,
                                 attention_mask=b_input_mask,
                                 labels=b_labels).to_tuple()

            total_train_loss += loss.item()
            loss.backward()
            optimizer.step()
            scheduler.step()

            if step % 5 == 0 and step != 0:
                elapsed = format_time(time.time() - t0)
                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))
                avg_train_loss = total_train_loss / step
                wandb.log({'train_loss': avg_train_loss})

            logits = logits.detach().cpu().numpy()
            label_ids = b_labels.cpu().numpy()
            total_train_accuracy += accuracy(logits, label_ids)

        avg_train_loss = total_train_loss / len(train_dataloader)
        avg_train_accuracy = total_train_accuracy / len(train_dataloader)
        print("  accuracy: {0:.2f}".format(avg_train_accuracy))
        training_time = format_time(time.time() - t0)

        wandb.log({'avg_train_accuracy': avg_train_accuracy,
                   'avg_train_loss': avg_train_loss,
                   'epochs': epoch_i,})

        print("")
        print("  average training loss: {0:.3f}".format(avg_train_loss))
        print("  training epoch took: {:}".format(training_time))

        print("")
        print("Running testing...")

        t0 = time.time()
        model.eval()

        total_test_accuracy = 0
        total_test_loss = 0

        for batch in validation_dataloader:
            b_input_ids = batch[0].to(device)
            b_input_mask = batch[1].to(device)
            b_labels = batch[2].to(device)

            with torch.no_grad():
                loss, logits = model(b_input_ids,
                                     attention_mask=b_input_mask,
                                     labels=b_labels).to_tuple()

                total_test_loss += loss.item()

            logits = logits.detach().cpu().numpy()
            label_ids = b_labels.cpu().numpy()
            total_test_accuracy += accuracy(logits, label_ids)

        avg_test_accuracy = total_test_accuracy / len(validation_dataloader)
        print("  accuracy: {0:.4f}".format(avg_test_accuracy))

        if avg_test_accuracy > best_accuracy:
            best_accuracy = avg_test_accuracy
            save_path = f"best_model_epoch_{epoch_i + 1}.pth"
            # torch.save(model.state_dict(), save_path)
            print(f"New best model saved to {save_path} with accuracy {best_accuracy:.4f}")
        save_path = f"model_epoch_{epoch_i + 1}.pth"
        # torch.save(model.state_dict(), save_path)
        print(f"Model saved to {save_path} with accuracy {avg_test_accuracy:.4f}")

        testing_time = format_time(time.time() - t0)
        wandb.log({'avg_test_accuracy': avg_test_accuracy, 'epochs': epoch_i,})
        print("  testing took: {:}".format(testing_time))

    print("training complete!")
    print("total training took {:} (h:mm:ss)".format(format_time(time.time()-total_t0)))
    print("best accuracy: ", best_accuracy)

# Define arguments
args = {
    'save_path': './',
    'model': 'microsoft/deberta-v3-large',
    'bs': 16,
    'epochs': 20,
    'lr': 1e-5,
    'aug': False,
    'debug': False
}

# Initialize wandb
wandb.login()
wandb.init(project="1508", entity="distill-llms", config={"learning_rate": args['lr'], "epochs": args['epochs'], "batch_size": args['bs']})

# Start training
train(args)

def predict_best_candidate(model, tokenizer, query, candidates, device):
    model.eval()

    # Encode each (query, candidate) pair
    inputs = [
        tokenizer(query, candidate,
                  truncation=True,
                  padding='max_length',
                  max_length=128,
                  return_tensors='pt')
        for candidate in candidates
    ]

    # Stack into tensors
    input_ids = torch.stack([item['input_ids'].squeeze(0) for item in inputs])  # (num_candidates, seq_len)
    attention_mask = torch.stack([item['attention_mask'].squeeze(0) for item in inputs])

    # Add batch dimension: (1, num_candidates, seq_len)
    input_ids = input_ids.unsqueeze(0).to(device)
    attention_mask = attention_mask.unsqueeze(0).to(device)

    # Forward pass
    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits  # shape: (1, num_candidates)
        predicted_idx = torch.argmax(logits, dim=1).item()

    return predicted_idx, logits.softmax(dim=1).squeeze().tolist()  # return index and probs

# Define test queries as a list of dictionaries
test_queries = [
    {
        "query": "I need a quick, protein-rich breakfast I can eat on the go.",
        "candidates": [
            "Greek yogurt with fruit and granola",
            "Pancakes with syrup",
            "Egg and spinach wrap",
            "Bagel with cream cheese",
            "Oatmeal with honey and nuts"
        ]
    },
    {
        "query": "I'm watching my weight but craving something savory for dinner.",
        "candidates": [
            "Grilled chicken salad with mixed greens",
            "Cheesy lasagna",
            "Pasta in a rich cream sauce",
            "Stir-fried tofu and vegetables",
            "Beef burger with fries"
        ]
    },
    {
        "query": "I want a hearty vegetarian lunch that's high in fiber.",
        "candidates": [
            "Quinoa salad with black beans and corn",
            "Veggie burger on a bun",
            "Spinach and ricotta stuffed ravioli",
            "Mushroom risotto",
            "Falafel wrap with hummus"
        ]
    },
    {
        "query": "I need a light, refreshing dessert that isn't too sweet.",
        "candidates": [
            "Mixed berry salad with mint",
            "Chocolate lava cake",
            "Lemon sorbet",
            "Cheesecake with strawberry topping",
            "Vanilla ice cream sundae"
        ]
    },
    {
        "query": "I'm looking for a comforting soup that's low in sodium and rich in vegetables.",
        "candidates": [
            "Homemade minestrone with extra vegetables",
            "Creamy tomato basil soup",
            "Beef stew",
            "Chicken noodle soup",
            "Vegetable lentil soup"
        ]
    }
]

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = AutoModelForMultipleChoice.from_pretrained(args['model']).to(device)
tokenizer = AutoTokenizer.from_pretrained(args['model'])
state_dict = torch.load("/content/best_model_epoch_8.pth", map_location=device)
model.load_state_dict(state_dict)

# Example usage: print each query and its candidate list
for i, test in enumerate(test_queries, start=1):
    print(f"Test Query {i}: {test['query']}")
    predicted_index, probs = predict_best_candidate(model, tokenizer, test['query'], test['candidates'], device)
    print(f"Predicted: {test['candidates'][predicted_index]}")
    print("All probabilities:")
    for i, (c, p) in enumerate(zip(test['candidates'], probs)):
        print(f"{i+1}. {c} — {p:.4f}")